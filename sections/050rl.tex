\section{Approach}
% Our training process consists of two stages: supervised fine-tuning (SFT) and reinforcement learning (RL). Experiments show that both stages are crucial for achieving optimal performance. The SFT stage provides a cold start for the model. After SFT, the model output becomes more readable, exhibits fewer hallucinations, and is less harmful. Models that undergo SFT demonstrate greater stability during the RL stage. Based on that,  our RL algorithm further unlocks the model's potential, enabling the final model to exhibit outstanding reasoning capabilities.

\subsection{Supervised Fine-Tuning}

Our training process starts with supervised fine-tuning (SFT). The SFT phase sets a solid foundation for the subsequent reinforcement learning stage. Compared to initiating RL from a base model, the SFT model produces more readable outputs, exhibits fewer instances of hallucination, and demonstrates reduced harmfulness. We curate an SFT data comprising 400k training instance, including 300k verifiable problems and 100k non-verifiable problems. Verifiable prompts are randomly sampled from RL training set. Non-verifiable data are sourced from the SFT data used for Doubao-Pro 1.5 \cite{doubao1.5pro}, covering areas such as creative writing, knowledge-based QA, safety, and function calling.

To generate high-quality responses with long CoT, we employ an iterative workflow that integrates model synthesis, human annotation, and rejection sampling. Initially, human experts apply prompt engineering techniques or engage in interactive dialogues with an internal model to produce responses with various reasoning patterns. After accumulating tens of high-quality cold-start samples, we can train a reasoning model with long CoT as a more capable assistant. Then we perform rejection sampling on this reasoning model using Seed-Verifier. While this workflow is primarily applied to mathematical data, we observe it can generalize well to other domains, such as coding, logic puzzle and even creative writing. Thus, for other domains, we also conduct a cold start process followed by rejection sampling to produce detailed reasoning trajectories.

During training, each instance is truncated to 32,000 tokens. We fine-tune the base model for two epochs using the above data. We use a cosine decay learning rate scheduling that the peak lr is $2 \times 10^{-5}$ and decays to $2 \times 10^{-6}$ gradually. 
 
% Unlike objective problems, the subjective dataset comprises prompt-response pairs labeled by human annotators. To enrich these data with reasoning processes, we use reasoning models trained on objective domains to generate detailed reasoning paths for the subjective responses. Then human labelers and a reward model evaluate these augmented responses. 

% To produce high-quality responses with long COT, we employ a hybrid approach that combines model synthesis, human annotation and rejection sampling. For each domain, we select a specific model to generate reasoning trajectories and assist human experts during annotation. Experts apply prompt engineering techniques or engage in interactive dialogues with the model to produce responses with varied reasoning patterns, which are then carefully filtered and refined. In addition, we use Seed-Verifier, Seed-Thinking-Verifier, code sandbox and a general reward model to evaluate the response quality and perform rejection sampling on the data. This iterative workflow enables continuous improvement. For example, after accumulating several hundred high-quality samples, we can retrain an SFT model to serve as a more capable assistant for further annotations.



\subsection{Reinforcement Learning}

We have developed a unified reinforcement learning framework that seamlessly fuses data from a broad range of domains. This integration incorporates three data categories:
\begin{itemize}
    \item Verifiable data, which obtains feedback from a verifier. This type of data allows for direct validation of the model's outputs against known criteria.
    \item General data, scored by a reward model. The reward model assigns scores based on how well the model's responses align with human preferences.
    \item A specific class of data that combines scores from both the verifier and the reward model. This hybrid data type leverages the strengths of both verification and reward-based evaluation.
\end{itemize}
% For data relying on reward models, we utilize the generative pairwise reward model presented in \citep{qrl}.

In the context of long-CoT RLHF, we encounter several challenges such as value model bias and the sparsity of reward signals. To address these issues, we draw on key techniques from our prior work \cite{yuan2025s, dapo, vapo}:
\begin{itemize}
    \item \textbf{Value-Pretraining}: We sample responses from a fixed policy, such as $\pi_{\text{sft}}$, and update the value model using the Monte-Carlo return. This process ensures that the initialized value model is fully aligned with our policy $\pi_{\text{sft}}$. Maintaining this alignment has been proven to be crucial for preserving the model's CoT pattern, enabling the model to generate coherent and logical CoT.
    \item \textbf{Decoupled-GAE}: By employing different Generalized Advantage Estimation (GAE) parameters, such as $\lambda_{\text{value}} = 1.0$ and $\lambda_{\text{policy}} = 0.95$, we allow the value model to update in an unbiased manner. Meanwhile, the policy can independently balance its own bias and variance. This decoupling enables more efficient and stable training of the model.
    \item \textbf{Length-adaptive GAE}: We set $\lambda_{\text{policy}} = 1-\frac{1}{\alpha l}$, where $\alpha$ is a hyper-parameter and $l$ is the response length. This approach ensures a more uniform distribution of Temporal Difference (TD) errors across both short and long sequences. As a result, the model can handle sequences of varying lengths more effectively during training.
    \item \textbf{Dynamic Sampling}: We employ dynamic sampling and filter out prompts with accuracy scores equal to 1 or 0, retaining only those in the batch that exhibit effective gradients. This process helps prevent the dampening of gradient signals during model training.
    \item \textbf{Clip-Higher}: In the Proximal Policy Optimization (PPO) algorithm, we decouple the upper and lower clip bounds as follows:
    \begin{equation}
\mathcal{L}^{CLIP}(\theta)=\hat{\mathbb{E}}_t\left[\min\left(r_t(\theta)\hat{A}_t,\text{clip}(r_t(\theta), 1-\epsilon_\text{low}, 1+\epsilon_\text{high})\hat{A}_t\right)\right]
\end{equation}
By increasing the value of $\epsilon_\text{high}$, we create more room for the increase of low-probability tokens. This encourages the model to explore a wider range of possible responses, enhancing its ability to discover novel and effective solutions.
    \item \textbf{Token-level Loss}: Instead of defining the policy loss over entire responses, we define it over all tokens. This approach addresses the imbalance in the token-level contribution to the final loss, ensuring that each token's impact on the training process is appropriately accounted for.
    \item \textbf{Positive Example LM Loss}: This loss function is designed to boost the utilization efficiency of positive samples during the RL training process. We add a language model loss with a coefficient $\mu$ for positive examples:
    \begin{align}
    \mathcal{L}(\theta) = \mathcal{L}_{\text{PPO}}(\theta)+\mu * \mathcal{L}_{\text{NLL}}(\theta)
    \end{align}
This additional loss term helps the model to better learn from positive examples, improving its overall performance.
\end{itemize}

When merging data from different domains and incorporating diverse scoring mechanisms, we face the challenge of interference between different data domains. This interference can arise from disparities in difficulty levels, the risk of reward-hacking, and other underlying factors. These issues make it extremely difficult to achieve uniform and simultaneous improvements across all capabilities of the model. To counteract this, we introduce \textbf{Online Data Distribution Adaptation}. This method transforms the stationary prompt distribution during reinforcement learning into an adaptive distribution that better caters to the model's requirements during training. By doing so, we minimize the negative impact of data interference and ensure a more balanced improvement across different abilities. As a result, the model can enhance its performance more consistently across a wide array of tasks.

%To handle the potentially extremely long responses generated from an extended chain of thought, we have implemented an asynchronous generation mechanism. This mechanism ensures that the computational time required for response generation does not block the training phase. Moreover, on the algorithmic side, we apply distribution correction to account for the off-policyness introduced by the asynchronous generation. This measure helps maintain the integrity of the training process, even when using a non-standard data generation approach. 


% Through the seamless merging of data from different domains, we have achieved simultaneous enhancement of a broad spectrum of capabilities. For instance, we have witnessed significant improvements in mathematical reasoning and creative writing, among other skills. During the RL training process, we have effectively tackled two major challenges. Firstly, we have minimized the adverse effects of data interference originating from different data sources. This has led to a more stable and reliable learning environment. Secondly, we have overcome the difficulty of optimizing data with highly variable response lengths, which are often caused by different prompt types. This has enabled us to train the model more effectively and obtain more consistent results.

%During the reinforcement learning (RL) phases, we implement a multi-stage RL approach to mitigate the data conflicts stemming from diverse domains, like mathematical reasoning and creative writing. Through empirical analysis, we have discovered that there is a clear separation in prompts for data from different domains. When training RL on a new data domain, it has minimal adverse effects on the performance in the previously trained domain. This is in contrast to the patterns witnessed during supervised fine-tuning (SFT), where cross-domain interference is often more pronounced.
%In this stage, our emphasis is on improving more generalized capabilities that cannot be attained merely through verifier-based reward. Examples of such capabilities include instruction-following and creative writing. Therefore, we mix both verifier-based dataset (i.e. math) and reward-model-based dataset (i.e. instruction-following). We also follow the pairwise reward model technique proposed in \citep{qrl} to alleviate reward-hacking.

%For RL training, we also incorporate the techniques presented in VC-PPO \citep{yuan2025s} and VAPO \cite{}. To guarantee that the model adheres to a specific Chain-of-Thought (CoT) format, a format penalty is imposed whenever the model deviates from this format. Additionally, the CoT part is masked during the scoring process of the reward model. 




