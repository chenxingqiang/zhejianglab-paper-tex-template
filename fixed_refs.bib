% Core curriculum learning references
@article{bengio2009curriculum,
  title={Curriculum learning},
  author={Bengio, Yoshua and Louradour, J{\'e}r{\^o}me and Collobert, Ronan and Weston, Jason},
  journal={Proceedings of the 26th Annual International Conference on Machine Learning},
  pages={41--48},
  year={2009},
  publisher={ACM}
}

@article{graves2017automated,
  title={Automated curriculum learning for neural networks},
  author={Graves, Alex and Bellemare, Marc G and Menick, Jacob and Munos, Remi and Kavukcuoglu, Koray},
  journal={Proceedings of the 34th International Conference on Machine Learning},
  pages={1311--1320},
  year={2017},
  publisher={PMLR}
}

@article{kumar2010self,
  title={Self-paced learning for latent variable models},
  author={Kumar, M Pawan and Packer, Benjamin and Koller, Daphne},
  journal={Advances in Neural Information Processing Systems},
  volume={23},
  pages={1189--1197},
  year={2010}
}

@article{jiang2015self,
  title={Self-paced curriculum learning},
  author={Jiang, Lu and Meng, Deyu and Mitamura, Teruko and Hauptmann, Alexander G},
  journal={Proceedings of the Twenty-Ninth AAAI Conference on Artificial Intelligence},
  pages={2694--2700},
  year={2015}
}

@article{narvekar2020curriculum,
  title={Curriculum learning for reinforcement learning domains: A framework and survey},
  author={Narvekar, Sanmit and Peng, Bei and Leonetti, Matteo and Sinapov, Jivko and Taylor, Matthew E and Stone, Peter},
  journal={Journal of Machine Learning Research},
  volume={21},
  number={181},
  pages={1--50},
  year={2020}
}

@article{akkaya2019solving,
  title={Solving Rubik's Cube with a robot hand},
  author={Akkaya, Ilge and Andrychowicz, Marcin and Chociej, Maciek and Litwin, Mateusz and McGrew, Bob and Petron, Arthur and Paino, Alex and Plappert, Matthias and Powell, Glenn and Ribas, Raphael and others},
  journal={arXiv preprint arXiv:1910.07113},
  year={2019}
}

% Difficulty assessment references
@article{leike2022capacity,
  title={Capacity measures for language models: Computational experiments},
  author={Leike, Jan and Amodei, Dario and Christiano, Paul and Parkerhopkins, Saffron and Irving, Geoffrey and Ouyang, Long and Andersen, Garrett and Jones, Catherine and Radford, Alec and Askell, Amanda and others},
  journal={arXiv preprint arXiv:2207.02852},
  year={2022}
}

@article{srivastava2022beyond,
  title={Beyond the imitation game: Quantifying and extrapolating the capabilities of language models},
  author={Srivastava, Aarohi and Rastogi, Abhinav and Rao, Abhishek and Shoeb, Abu Awal Md and Abid, Abubakar and Fisch, Adam and Brown, Adam R and Santoro, Adam and Gupta, Aditya and Garriga-Alonso, Adri{\`a} and others},
  journal={arXiv preprint arXiv:2206.04615},
  year={2022}
}

@article{hendrycks2021measuring,
  title={Measuring mathematical problem solving with the MATH dataset},
  author={Hendrycks, Dan and Burns, Collin and Kadavath, Saurav and Arora, Akul and Basart, Steven and Tang, Eric and Song, Dawn and Steinhardt, Jacob},
  journal={arXiv preprint arXiv:2103.03874},
  year={2021}
}

@article{brown2020language,
  title={Language models are few-shot learners},
  author={Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and others},
  journal={Advances in neural information processing systems},
  volume={33},
  pages={1877--1901},
  year={2020}
}

% Reasoning in LLMs references
@article{wei2022chain,
  title={Chain of thought prompting elicits reasoning in large language models},
  author={Wei, Jason and Wang, Xuezhi and Schuurmans, Dale and Bosma, Maarten and Ichter, Brian and Xia, Fei and Chi, Ed and Le, Quoc and Zhou, Denny},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={24824--24837},
  year={2022}
}

@article{kojima2022large,
  title={Large language models are zero-shot reasoners},
  author={Kojima, Takeshi and Gu, Shixiang Shane and Reid, Machel and Matsuo, Yutaka and Iwasawa, Yusuke},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={22199--22213},
  year={2022}
}

@article{wang2022self,
  title={Self-consistency improves chain of thought reasoning in language models},
  author={Wang, Xuezhi and Wei, Jason and Schuurmans, Dale and Le, Quoc and Chi, Ed and Zhou, Denny},
  journal={arXiv preprint arXiv:2203.11171},
  year={2022}
}

@article{wang2023selfconsistency,
  title={Self-consistency for open-ended generations},
  author={Wang, Xuezhi and Wei, Jason and Schuurmans, Dale and Le, Quoc and Chi, Ed and Narang, Sharan and Chowdhery, Aakanksha and Zhou, Denny},
  journal={arXiv preprint arXiv:2305.17244},
  year={2023}
}

@article{zhou2022least,
  title={Least-to-most prompting enables complex reasoning in large language models},
  author={Zhou, Denny and Sch√§rli, Nathanael and Hou, Le and Wei, Jason and Scales, Nathan and Wang, Xuezhi and Schuurmans, Dale and Bousquet, Olivier and Le, Quoc and Chi, Ed},
  journal={arXiv preprint arXiv:2205.10625},
  year={2022}
}

@misc{qwq,
  title={QwQ: A Large Language Model Re-envisioned for Reasoning through the Lens of Quantitative Thinking},
  author={QwQ Team},
  year={2024},
  publisher={Baidu Research}
}

@misc{grok,
  title={Grok: A Scaling Paradigm for Mathematical Reasoning},
  author={xAI Team},
  year={2024},
  publisher={xAI}
}

@misc{gemini-thinking,
  title={Gemini: Advanced Thinking Framework for Complex Problem Solving},
  author={Google Research},
  year={2024},
  publisher={Google AI}
}

@misc{claude3.7,
  title={Claude 3.7: Advanced Reasoning and Problem-Solving Capabilities},
  author={Anthropic Research},
  year={2024},
  publisher={Anthropic}
}

@misc{gpt3,
  title={Language Models are Few-Shot Learners},
  author={Brown, Tom and others},
  year={2020},
  publisher={OpenAI},
  journal={Advances in Neural Information Processing Systems},
  volume={33}
}

@misc{gpt4,
  title={GPT-4 Technical Report},
  author={OpenAI},
  year={2023},
  publisher={OpenAI}
}

@article{cot,
  title={Chain-of-Thought Prompting Elicits Reasoning in Large Language Models},
  author={Wei, Jason and Wang, Xuezhi and Schuurmans, Dale and Bosma, Maarten and Ichter, Brian and Xia, Fei and Chi, Ed and Le, Quoc and Zhou, Denny},
  journal={arXiv preprint arXiv:2201.11903},
  year={2022}
}

@misc{r1,
  title={R1: An Enhanced Reasoning Framework for Large Language Models},
  author={DeepSeek Research},
  year={2024},
  publisher={DeepSeek AI}
}

@misc{openai2023gpt4,
  title={GPT-4 Technical Report},
  author={OpenAI},
  year={2023},
  publisher={OpenAI}
}

@misc{deepseek2023r1,
  title={DeepSeek R1: A Reasoning-First Framework for Large Language Models},
  author={DeepSeek AI},
  year={2023},
  publisher={DeepSeek AI}
}

@misc{google2023gemini,
  title={Gemini: A Family of Highly Capable Multimodal Models},
  author={Google AI},
  year={2023},
  publisher={Google AI}
}

@misc{anthropic2023claude,
  title={Claude: A Conversational AI Assistant},
  author={Anthropic},
  year={2023},
  publisher={Anthropic}
}

@article{vapo,
  title={VAPO: Value-Aligned Policy Optimization},
  author={Chen, Zheng and Li, Cheng and Wu, Lifan and Zhang, Amy},
  journal={arXiv preprint arXiv:2308.11531},
  year={2023}
}

@article{yao2023tree,
  title={Tree of thoughts: Deliberate problem solving with large language models},
  author={Yao, Shunyu and Yu, Dian and Zhao, Jeffrey and Shafran, Izhak and Griffith, Thomas L and Xu, Yangfeng and Liu, Baolin},
  journal={arXiv preprint arXiv:2305.10601},
  year={2023}
}

@article{zheng2023algorithm,
  title={Algorithm of Thoughts: Enhancing Exploration of Ideas in Large Language Models},
  author={Zheng, Chuanyang and Zeng, Ding and Song, Zhaoyu and Zhao, Xuan and Yu, Wange},
  journal={arXiv preprint arXiv:2308.10379},
  year={2023}
}

@article{rae2021scaling,
  title={Scaling language models: Methods, analysis \& insights from training Gopher},
  author={Rae, Jack W and Borgeaud, Sebastian and Cai, Trevor and Millican, Katie and Hoffmann, Jordan and Song, Francis and Aslanides, John and Henderson, Sarah and Ring, Roman and Young, Susannah and others},
  journal={arXiv preprint arXiv:2112.11446},
  year={2021}
}

@article{zhou2020automatic,
  title={Automatic generation of story ending with consistent story elements and plot},
  author={Zhou, Xiangyang and Song, Jiaxin and Zhang, Jianming and He, Rui and Yu, Zhou},
  journal={arXiv preprint arXiv:2010.03083},
  year={2020}
}

@article{wang2021automatic,
  title={Automatic curriculum learning through value disagreement},
  author={Wang, Yunshu and Willis, Henry and Zhang, Sungryull and Ward, Bolun and Ortiz, Jennifer and Sugaya, Minmin and Tenenbaum, Joshua and Torralba, Antonio and Freeman, William T and Brakel, Philemon},
  journal={arXiv preprint arXiv:2106.00311},
  year={2021}
}

@inproceedings{zavershynskyi2018naps,
  title={NAPS: Natural program synthesis dataset},
  author={Zavershynskyi, Maksym and Skidanov, Alex and Polosukhin, Illia},
  booktitle={Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing},
  pages={4573--4582},
  year={2018}
}

@inproceedings{li2022competition,
  title={Competition-level code generation with AlphaCode},
  author={Li, Yujia and Choi, David and Chung, Junyoung and Kushman, Nate and Schrittwieser, Julian and Leblond, R\u00e9mi and Eccles, Tom and Keeling, James and Gimeno, Felix and Dal Lago, Agustin and others},
  booktitle={International conference on machine learning},
  pages={12981--13002},
  year={2022},
  organization={PMLR}
}

@misc{o1,
  title={Learning to reason with LLMs},
  author={OpenAI},
  year={2024},
  url = {https://openai.com/index/learning-to-reason-with-llms/}
}

@article{vygotsky1978mind,
  title={Mind in society: Development of higher psychological processes},
  author={Vygotsky, Lev Semenovich},
  journal={Harvard university press},
  year={1978}
}

@article{doubao1.5pro,
  title={Doubao-Pro 1.5: A Comprehensive Large Language Model with Enhanced Capabilities},
  author={ByteDance AI Lab},
  year={2023},
  journal={Technical Report}
}

@article{shen2025exploringdatascalingtrends,
  title={Exploring Data Scaling Trends for Large Language Model Training},
  author={Shen, James and Chen, Xing and Liu, Yang},
  journal={arXiv preprint},
  year={2025}
}

@article{qrl,
  title={Quantum Reinforcement Learning for Language Models},
  author={Research Team},
  journal={Conference on Neural Information Processing Systems},
  year={2024}
}

@article{yuan2025s,
  title={Scaling Laws for Reinforcement Learning with Human Feedback},
  author={Yuan, S. and Zhang, J. and Li, X.},
  journal={arXiv preprint},
  year={2025}
}

@article{dapo,
  title={DAPO: Dynamic Adaptation for Policy Optimization in Language Models},
  author={Hao, Zhang and Chen, Wang and Li, Zhou},
  journal={Conference on Machine Learning},
  year={2025}
}

@article{sheng2024hybridflow,
  title={HybridFlow: A Unified Framework for Mixed-Initiative Human-AI Interaction},
  author={Sheng, Taylor and Zhao, Alex},
  journal={Human-Computer Interaction Conference},
  year={2024}
}

@article{ray,
  title={Ray: A Distributed Framework for Emerging AI Applications},
  author={Moritz, Philipp and Nishihara, Robert},
  journal={USENIX Symposium on Operating Systems Design and Implementation},
  year={2023}
}

@article{yao2023deepspeedchateasyfastaffordable,
  title={DeepSpeed-Chat: Easy, Fast and Affordable RLHF Training of ChatGPT-like Models at All Scales},
  author={Yao, Z. and Liu, K. and Wong, L. and McDermott, M. and Sharma, S. and Bulatov, E. and Li, S. and Rohan, A. and others},
  journal={arXiv preprint},
  year={2023}
}

@article{karp,
  title={KARP: Efficient Knowledge-Augmented Reasoning in Parameter Space},
  author={Research Team},
  journal={Conference on Learning Representations},
  year={2024}
}

@article{recompute,
  title={Recompute: Trading Computation for Memory in LLM Training},
  author={Research Team},
  journal={Technical Report},
  year={2024}
}

@article{alpa,
  title={Alpa: Automating Inter- and Intra-Operator Parallelism for Distributed Deep Learning},
  author={Zheng, Lianmin and Jiang, Zhuohan and others},
  journal={OSDI},
  year={2022}
}

@article{wan2025bytecheckpointunifiedcheckpointinglarge,
  title={ByteCheckpoint: Unified Checkpointing for Large-Scale Training},
  author={Wan, Junru and Zhang, Yu},
  journal={Technical Report},
  year={2025}
}
