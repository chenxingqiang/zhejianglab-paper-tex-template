% Core curriculum learning references
@article{bengio2009curriculum,
  title={Curriculum learning},
  author={Bengio, Yoshua and Louradour, J{\'e}r{\^o}me and Collobert, Ronan and Weston, Jason},
  journal={Proceedings of the 26th Annual International Conference on Machine Learning},
  pages={41--48},
  year={2009},
  publisher={ACM}
}

@article{graves2017automated,
  title={Automated curriculum learning for neural networks},
  author={Graves, Alex and Bellemare, Marc G and Menick, Jacob and Munos, Remi and Kavukcuoglu, Koray},
  journal={Proceedings of the 34th International Conference on Machine Learning},
  pages={1311--1320},
  year={2017},
  publisher={PMLR}
}

@article{kumar2010self,
  title={Self-paced learning for latent variable models},
  author={Kumar, M Pawan and Packer, Benjamin and Koller, Daphne},
  journal={Advances in Neural Information Processing Systems},
  volume={23},
  pages={1189--1197},
  year={2010}
}

@article{jiang2015self,
  title={Self-paced curriculum learning},
  author={Jiang, Lu and Meng, Deyu and Mitamura, Teruko and Hauptmann, Alexander G},
  journal={Proceedings of the Twenty-Ninth AAAI Conference on Artificial Intelligence},
  pages={2694--2700},
  year={2015}
}

@article{narvekar2020curriculum,
  title={Curriculum learning for reinforcement learning domains: A framework and survey},
  author={Narvekar, Sanmit and Peng, Bei and Leonetti, Matteo and Sinapov, Jivko and Taylor, Matthew E and Stone, Peter},
  journal={Journal of Machine Learning Research},
  volume={21},
  number={181},
  pages={1--50},
  year={2020}
}

@article{akkaya2019solving,
  title={Solving Rubik's Cube with a robot hand},
  author={Akkaya, Ilge and Andrychowicz, Marcin and Chociej, Maciek and Litwin, Mateusz and McGrew, Bob and Petron, Arthur and Paino, Alex and Plappert, Matthias and Powell, Glenn and Ribas, Raphael and others},
  journal={arXiv preprint arXiv:1910.07113},
  year={2019}
}

% Difficulty assessment references
@article{leike2022capacity,
  title={Capacity measures for language models: Computational experiments},
  author={Leike, Jan and Amodei, Dario and Christiano, Paul and Parkerhopkins, Saffron and Irving, Geoffrey and Ouyang, Long and Andersen, Garrett and Jones, Catherine and Radford, Alec and Askell, Amanda and others},
  journal={arXiv preprint arXiv:2207.02852},
  year={2022}
}

@article{srivastava2022beyond,
  title={Beyond the imitation game: Quantifying and extrapolating the capabilities of language models},
  author={Srivastava, Aarohi and Rastogi, Abhinav and Rao, Abhishek and Shoeb, Abu Awal Md and Abid, Abubakar and Fisch, Adam and Brown, Adam R and Santoro, Adam and Gupta, Aditya and Garriga-Alonso, Adri{\`a} and others},
  journal={arXiv preprint arXiv:2206.04615},
  year={2022}
}

@article{hendrycks2021measuring,
  title={Measuring mathematical problem solving with the MATH dataset},
  author={Hendrycks, Dan and Burns, Collin and Kadavath, Saurav and Arora, Akul and Basart, Steven and Tang, Eric and Song, Dawn and Steinhardt, Jacob},
  journal={arXiv preprint arXiv:2103.03874},
  year={2021}
}

@article{brown2020language,
  title={Language models are few-shot learners},
  author={Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and others},
  journal={Advances in neural information processing systems},
  volume={33},
  pages={1877--1901},
  year={2020}
}

% Reasoning in LLMs references
@article{wei2022chain,
  title={Chain of thought prompting elicits reasoning in large language models},
  author={Wei, Jason and Wang, Xuezhi and Schuurmans, Dale and Bosma, Maarten and Ichter, Brian and Xia, Fei and Chi, Ed and Le, Quoc and Zhou, Denny},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={24824--24837},
  year={2022}
}

@article{kojima2022large,
  title={Large language models are zero-shot reasoners},
  author={Kojima, Takeshi and Gu, Shixiang Shane and Reid, Machel and Matsuo, Yutaka and Iwasawa, Yusuke},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={22199--22213},
  year={2022}
}

@article{wang2022self,
  title={Self-consistency improves chain of thought reasoning in language models},
  author={Wang, Xuezhi and Wei, Jason and Schuurmans, Dale and Le, Quoc and Chi, Ed and Zhou, Denny},
  journal={arXiv preprint arXiv:2203.11171},
  year={2022}
}

@article{wang2023selfconsistency,
  title={Self-consistency for open-ended generations},
  author={Wang, Xuezhi and Wei, Jason and Schuurmans, Dale and Le, Quoc and Chi, Ed and Narang, Sharan and Chowdhery, Aakanksha and Zhou, Denny},
  journal={arXiv preprint arXiv:2305.17244},
  year={2023}
}

@article{zhou2022least,
  title={Least-to-most prompting enables complex reasoning in large language models},
  author={Zhou, Denny and Sch√§rli, Nathanael and Hou, Le and Wei, Jason and Scales, Nathan and Wang, Xuezhi and Schuurmans, Dale and Bousquet, Olivier and Le, Quoc and Chi, Ed},
  journal={arXiv preprint arXiv:2205.10625},
  year={2022}
}

@article{yao2023tree,
  title={Tree of thoughts: Deliberate problem solving with large language models},
  author={Yao, Shunyu and Yu, Dian and Zhao, Jeffrey and Shafran, Izhak and Griffiths, Thomas L and Cao, Yuan and Narasimhan, Karthik},
  journal={arXiv preprint arXiv:2305.10601},
  year={2023}
}

@article{zheng2023algorithm,
  title={Algorithm of thoughts: Enhancing exploration of ideas in large language models},
  author={Zheng, Cheng and Cui, Wangchunshu and Zhang, Yizhe and Bian, Jianwei and Rose, Sharon and Lee, Ellie and Yao, Zizhen and Liu, Dongdong and Chen, Eric and Yang, He and others},
  journal={arXiv preprint arXiv:2308.10379},
  year={2023}
}

@article{rae2021scaling,
  title={Scaling language models: Methods, analysis \& insights from training gopher},
  author={Rae, Jack W and Borgeaud, Sebastian and Cai, Trevor and Millican, Katie and Hoffmann, Jordan and Song, Francis and Aslanides, John and Henderson, Sarah and Ring, Roman and Young, Susannah and others},
  journal={arXiv preprint arXiv:2112.11446},
  year={2021}
}

% SOTA models references
@article{openai2023gpt4,
  title={GPT-4 Technical Report},
  author={OpenAI},
  journal={arXiv preprint arXiv:2303.08774},
  year={2023}
}

@article{deepseek2023r1,
  title={DeepSeek-Reasoner: Towards Cloud-Centric Reasoning for Large Language Models},
  author={DeepSeek-AI},
  journal={arXiv preprint arXiv:2402.07810},
  year={2023}
}

@article{google2023gemini,
  title={Gemini: A Family of Highly Capable Multimodal Models},
  author={Google DeepMind},
  journal={arXiv preprint arXiv:2312.11805},
  year={2023}
}

@article{anthropic2023claude,
  title={Claude: A Foundation Language Model Optimized For Dialog And Tool Use},
  author={Anthropic},
  journal={arXiv preprint arXiv:2401.08252},
  year={2023}
}

% Problem generation references
@article{zhou2020automatic,
  title={Automatic generation of mathematical word problems for mathematical education},
  author={Zhou, Ke and Yang, Si-Qing and Zha, Hui},
  journal={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={34},
  number={09},
  pages={13609--13616},
  year={2020}
}

@article{wang2021automatic,
  title={Automatic generation of math word problems via constrained neural networks},
  author={Wang, Tianyu and Miao, Xing and Luo, Wenjie and Wang, Yuchuan and Jiang, Yuke and Wu, Lei},
  journal={Knowledge-Based Systems},
  volume={225},
  pages={107091},
  year={2021},
  publisher={Elsevier}
}

@article{zavershynskyi2018naps,
  title={NAPS: Natural program synthesis dataset},
  author={Zavershynskyi, Miltiadis and Skachkov, Alex and Snover, Al and Johnson, Kyle},
  journal={arXiv preprint arXiv:1807.03168},
  year={2018}
}

@article{li2022competition,
  title={Competition-level code generation with alphacode},
  author={Li, Yujia and Choi, David and Chung, Junyoung and Kushman, Nate and Schrittwieser, Julian and Leblond, R{\'e}mi and Eccles, Tom and Keeling, James and Gimeno, Felix and Dal Lago, Agustin and others},
  journal={Science},
  volume={378},
  number={6624},
  pages={1092--1097},
  year={2022},
  publisher={American Association for the Advancement of Science}
}

% Educational psychology reference
@book{vygotsky1978mind,
  title={Mind in society: Development of higher psychological processes},
  author={Vygotsky, Lev Semenovich},
  year={1978},
  publisher={Harvard university press}
}
